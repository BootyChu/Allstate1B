{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoshiAligina/Allstate1B/blob/main/allstate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Drive Mounting & Imports**"
      ],
      "metadata": {
        "id": "2TEPtDeMq-U-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INcF2CesozzH",
        "outputId": "6ac6ac7f-4682-4a4b-842a-b1b09243ab85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Allstate1B/claims_data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "BHerjDENrGfd",
        "outputId": "76bf36af-e1ce-49b4-b03c-efeeb965f08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...     cont6     cont7  \\\n",
              "0   1    A    B    A    B    A    A    A    A    B  ...  0.718367  0.335060   \n",
              "1   2    A    B    A    A    A    A    A    A    B  ...  0.438917  0.436585   \n",
              "2   5    A    B    A    A    B    A    A    A    B  ...  0.289648  0.315545   \n",
              "3  10    B    B    A    B    A    A    A    A    B  ...  0.440945  0.391128   \n",
              "4  11    A    B    A    B    A    A    A    A    B  ...  0.178193  0.247408   \n",
              "\n",
              "     cont8    cont9   cont10    cont11    cont12    cont13    cont14     loss  \n",
              "0  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493  0.714843  2213.18  \n",
              "1  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431  0.304496  1283.60  \n",
              "2  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709  0.774425  3005.09  \n",
              "3  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077  0.602642   939.85  \n",
              "4  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011  0.432606  2763.85  \n",
              "\n",
              "[5 rows x 132 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e7e4ab3-20eb-4122-8290-75dd663602ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cat1</th>\n",
              "      <th>cat2</th>\n",
              "      <th>cat3</th>\n",
              "      <th>cat4</th>\n",
              "      <th>cat5</th>\n",
              "      <th>cat6</th>\n",
              "      <th>cat7</th>\n",
              "      <th>cat8</th>\n",
              "      <th>cat9</th>\n",
              "      <th>...</th>\n",
              "      <th>cont6</th>\n",
              "      <th>cont7</th>\n",
              "      <th>cont8</th>\n",
              "      <th>cont9</th>\n",
              "      <th>cont10</th>\n",
              "      <th>cont11</th>\n",
              "      <th>cont12</th>\n",
              "      <th>cont13</th>\n",
              "      <th>cont14</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.335060</td>\n",
              "      <td>0.30260</td>\n",
              "      <td>0.67135</td>\n",
              "      <td>0.83510</td>\n",
              "      <td>0.569745</td>\n",
              "      <td>0.594646</td>\n",
              "      <td>0.822493</td>\n",
              "      <td>0.714843</td>\n",
              "      <td>2213.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.438917</td>\n",
              "      <td>0.436585</td>\n",
              "      <td>0.60087</td>\n",
              "      <td>0.35127</td>\n",
              "      <td>0.43919</td>\n",
              "      <td>0.338312</td>\n",
              "      <td>0.366307</td>\n",
              "      <td>0.611431</td>\n",
              "      <td>0.304496</td>\n",
              "      <td>1283.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.289648</td>\n",
              "      <td>0.315545</td>\n",
              "      <td>0.27320</td>\n",
              "      <td>0.26076</td>\n",
              "      <td>0.32446</td>\n",
              "      <td>0.381398</td>\n",
              "      <td>0.373424</td>\n",
              "      <td>0.195709</td>\n",
              "      <td>0.774425</td>\n",
              "      <td>3005.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.440945</td>\n",
              "      <td>0.391128</td>\n",
              "      <td>0.31796</td>\n",
              "      <td>0.32128</td>\n",
              "      <td>0.44467</td>\n",
              "      <td>0.327915</td>\n",
              "      <td>0.321570</td>\n",
              "      <td>0.605077</td>\n",
              "      <td>0.602642</td>\n",
              "      <td>939.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>0.178193</td>\n",
              "      <td>0.247408</td>\n",
              "      <td>0.24564</td>\n",
              "      <td>0.22089</td>\n",
              "      <td>0.21230</td>\n",
              "      <td>0.204687</td>\n",
              "      <td>0.202213</td>\n",
              "      <td>0.246011</td>\n",
              "      <td>0.432606</td>\n",
              "      <td>2763.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 132 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e7e4ab3-20eb-4122-8290-75dd663602ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5e7e4ab3-20eb-4122-8290-75dd663602ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5e7e4ab3-20eb-4122-8290-75dd663602ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-17b2e977-46e7-4401-b5d7-c5925d372f3c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17b2e977-46e7-4401-b5d7-c5925d372f3c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-17b2e977-46e7-4401-b5d7-c5925d372f3c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_column = \"loss\"\n",
        "cont_feats =  ['cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n",
        "       'cont13', 'cont14']\n",
        "for feature in cont_feats:\n",
        "    sns.scatterplot(x=feature, y=label_column, data=df)\n",
        "    plt.title(f'Scatter plot of {feature} vs {label_column}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel(label_column)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uvpi1fpaNrCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_counts = df.isnull().sum()\n",
        "nan_counts\n",
        "label_column\n",
        "cat_feats= ['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n",
        "       'cat9']\n",
        "for feature in cat_feats:\n",
        "    sns.barplot(x=feature, y=label_column, data=df, estimator=sum)\n",
        "    plt.title(f'Bar plot of {feature} vs {label_column}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel(label_column)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Ldiy9-vhNr8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats = cont_feats + cat_feats\n",
        "\n",
        "df_encoded = pd.get_dummies(df[cat_feats])\n",
        "df_numeric = df[cont_feats]\n",
        "df_combined = pd.concat([df_numeric, df_encoded], axis=1)\n",
        "\n",
        "df_combined[label_column] = df[label_column]\n",
        "\n",
        "correlation_matrix = df_combined.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix[[label_column]].sort_values(by=label_column, ascending=False), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix for Loss Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "grJdQg2mNyce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical Feature Selection"
      ],
      "metadata": {
        "id": "Z-XjnAapN7Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_data = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "print(continuous_data.head())"
      ],
      "metadata": {
        "id": "t5BQabSeN1V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.select_dtypes(include=['float64', 'int64']).drop(columns=['id','loss'])\n",
        "y = df['loss']\n",
        "\n",
        "\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k='all')\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "\n",
        "feature_scores = selector.scores_\n",
        "\n",
        "\n",
        "feature_ranking = pd.DataFrame({'Feature': X.columns, 'Score': feature_scores})\n",
        "feature_ranking = feature_ranking.sort_values(by='Score', ascending=False)\n",
        "print(feature_ranking.to_string(index=False))"
      ],
      "metadata": {
        "id": "0vzBOK3EOAcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5\n",
        "top_features = feature_ranking.head(top_n)['Feature']\n",
        "\n",
        "top_features_list = top_features.tolist()\n",
        "\n",
        "print(\"Top 5 continuous features:\")\n",
        "print(top_features_list)"
      ],
      "metadata": {
        "id": "0gAJp6ieODcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Feature Selection"
      ],
      "metadata": {
        "id": "29JCsdlXN-GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
        "y = df_encoded[\"loss\"]\n",
        "X = df_encoded.drop(columns=['id', 'loss'], axis=1)\n",
        "# X\n",
        "# y"
      ],
      "metadata": {
        "id": "9inLbpYYOGl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "\n",
        "selected_features = X.columns[selected_indices]\n",
        "print(\"Selected Features:\", selected_features)\n",
        "\n",
        "X_kbest = X[selected_features]"
      ],
      "metadata": {
        "id": "lmNAZ8igOLNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "for column in categorical_columns:\n",
        "    mean_loss = df.groupby(column)['loss'].mean().reset_index()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=column, y='loss', data=mean_loss, color='lightblue', label='Mean Loss')\n",
        "\n",
        "\n",
        "    sns.lineplot(x=column, y='loss', data=df, color='red', marker='o', label='Actual Loss')\n",
        "\n",
        "    plt.title(f'Bar Graph of Mean Loss with Actual Loss Line for {column}')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GJM1qY-FONos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "significant_difference_columns = []\n",
        "\n",
        "for column in categorical_columns:\n",
        "\n",
        "    mean_loss = df.groupby(column)['loss'].mean().reset_index()\n",
        "\n",
        "\n",
        "    mean = mean_loss['loss'].mean()\n",
        "    std_dev = mean_loss['loss'].std()\n",
        "    cv = std_dev / mean if mean != 0 else 0\n",
        "\n",
        "    threshold = 0.4\n",
        "\n",
        "    if cv > threshold:\n",
        "        significant_difference_columns.append(column)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=column, y='loss', data=mean_loss, color='lightblue', label='Mean Loss')\n",
        "\n",
        "        sns.lineplot(x=column, y='loss', data=df, color='red', marker='o', label='Actual Loss')\n",
        "\n",
        "        plt.title(f'Bar Graph of Mean Loss with Actual Loss Line for {column}')\n",
        "        plt.legend()\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "print(\"Categorical columns with significant differences in mean loss:\")\n",
        "print(significant_difference_columns)"
      ],
      "metadata": {
        "id": "Bmwk4vroOR3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_cluster = [\"cat89\", \"cat92\",\"cat99\",\"cat101\", \"cat102\", \"cat105\", \"cat107\",'cat109', 'cat110',\"cat111\",'cat112', 'cat113', 'cat114', 'cat115', 'cat116']\n",
        "\n",
        "new_columns = [col for col in significant_difference_columns if col not in columns_to_cluster]\n",
        "\n",
        "print(\"Categorical columns with significant differences in mean loss to use:\")\n",
        "new_columns"
      ],
      "metadata": {
        "id": "rekahGezOUlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_columns=len(new_columns)\n",
        "\n",
        "number_of_columns"
      ],
      "metadata": {
        "id": "FawFb7KdOcob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def encode_and_cluster(df, column_name, n_clusters=5):\n",
        "    le = LabelEncoder()\n",
        "    df[f'{column_name}_encoded'] = le.fit_transform(df[column_name])\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters)\n",
        "    df[f'{column_name}_cluster'] = kmeans.fit_predict(df[[f'{column_name}_encoded']])\n",
        "\n",
        "    df[f'{column_name}_grouped'] = df[f'{column_name}_cluster'].map(lambda x: f'Group{x}')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "columns_to_cluster = [\"cat89\", \"cat92\",\"cat99\",\"cat101\", \"cat102\", \"cat105\", \"cat107\",'cat109', 'cat110',\"cat11\",'cat112', 'cat113', 'cat114', 'cat115', 'cat116']\n",
        "\n",
        "\n",
        "for column in columns_to_cluster:\n",
        "    df = encode_and_cluster(df, column, n_clusters=5)\n",
        "\n",
        "\n",
        "for column in columns_to_cluster:\n",
        "    grouped_column = f'{column}_grouped'\n",
        "    print(f\"\\nDistribution of {column} groups:\")\n",
        "    print(df[grouped_column].value_counts())"
      ],
      "metadata": {
        "id": "-dHSirkYOg8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mean_loss_by_group(df, grouped_column):\n",
        "\n",
        "    mean_loss = df.groupby(grouped_column)['loss'].mean().reset_index()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    sns.barplot(x=grouped_column, y='loss', data=mean_loss, color='lightblue', label='Mean Loss')\n",
        "\n",
        "\n",
        "    sns.lineplot(x=grouped_column, y='loss', data=mean_loss, color='red', marker='o', label='Actual Loss')\n",
        "\n",
        "\n",
        "    plt.title(f'Bar Graph of Mean Loss with Actual Loss Line for Hierarchical Groups ({grouped_column})')\n",
        "    plt.xlabel(f'Hierarchical Groups ({grouped_column})')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def encode_and_cluster(df, column_name, n_clusters=5):\n",
        "    le = LabelEncoder()\n",
        "    df[f'{column_name}_encoded'] = le.fit_transform(df[column_name])\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters)\n",
        "    df[f'{column_name}_cluster'] = kmeans.fit_predict(df[[f'{column_name}_encoded']])\n",
        "\n",
        "    df[f'{column_name}_grouped'] = df[f'{column_name}_cluster'].map(lambda x: f'Group{x}')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "columns_to_cluster = [\"cat89\", \"cat92\", \"cat99\", \"cat101\", \"cat102\", \"cat105\", \"cat107\", 'cat109', 'cat110', \"cat111\", 'cat112', 'cat113', 'cat114', 'cat115', 'cat116']\n",
        "\n",
        "for column in columns_to_cluster:\n",
        "    df = encode_and_cluster(df, column, n_clusters=5)\n",
        "\n",
        "\n",
        "for column in columns_to_cluster:\n",
        "    grouped_column = f'{column}_grouped'\n",
        "    if grouped_column in df.columns:\n",
        "        plot_mean_loss_by_group(df, grouped_column)"
      ],
      "metadata": {
        "id": "NlsVKnfxOjW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "grouped_cat = [\"cat92_grouped\",\"cat101_grouped\",\"cat111_grouped\",\"cat114_grouped\"]\n",
        "columns_to_include = new_columns + grouped_cat\n",
        "columns_to_include"
      ],
      "metadata": {
        "id": "0zj4j3PLOm_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Subset"
      ],
      "metadata": {
        "id": "Rgge_7zhOwrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_and_cluster(df, column_name, n_clusters=5):\n",
        "    \"\"\"Encodes and clusters the categorical column, returning the grouped version.\"\"\"\n",
        "    le = LabelEncoder()\n",
        "    df[f'{column_name}_encoded'] = le.fit_transform(df[column_name])\n",
        "\n",
        "    # Use KMeans clustering to group the encoded values\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    df[f'{column_name}_grouped'] = kmeans.fit_predict(df[[f'{column_name}_encoded']])\n",
        "\n",
        "    # Clean up temporary encoded column\n",
        "    df.drop([f'{column_name}_encoded'], axis=1, inplace=True)\n",
        "\n",
        "    return df[[f'{column_name}_grouped']]  # Return only the new grouped column"
      ],
      "metadata": {
        "id": "wmA-0PUiOygz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_cluster = [\n",
        "    \"cat89\", \"cat101\", \"cat102\", \"cat105\",\n",
        "    \"cat107\", 'cat113', 'cat115', 'cat116'\n",
        "]"
      ],
      "metadata": {
        "id": "ghFrGR_KO3ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an empty DataFrame to store grouped columns\n",
        "grouped_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "vK9pdDmQO7HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loop through each column, generate the grouped column, and add it to grouped_df\n",
        "for column in columns_to_cluster:\n",
        "    if column in df.columns:\n",
        "        grouped_col = encode_and_cluster(df, column)\n",
        "        grouped_df = pd.concat([grouped_df, grouped_col], axis=1)\n",
        "\n",
        "\n",
        "# Step 4: Add the top continuous and categroical features that did not need to be grouped into heirchies\n",
        "continuous_features = ['cont2', 'cont12', 'cont14', 'cont11', 'cont9']\n",
        "cat_features = ['cat7', 'cat57']\n",
        "\n",
        "\n",
        "# Encode cat7 and cat57 as numerical columns using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['cat7_encoded'] = le.fit_transform(df['cat7'])\n",
        "df['cat57_encoded'] = le.fit_transform(df['cat57'])\n",
        "encoded_cat_features = ['cat7_encoded', 'cat57_encoded']"
      ],
      "metadata": {
        "id": "oaC-YfD6O8QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.concat([df[continuous_features + encoded_cat_features], grouped_df], axis=1)\n",
        "\n",
        "# Check columns and data types\n",
        "print(\"Columns in the final DataFrame:\", df_final.columns)\n",
        "print(df_final.dtypes)"
      ],
      "metadata": {
        "id": "5lbqtoZLPAKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verify that grouped columns were added\n",
        "print(\"Columns in dataset after adding grouped columns:\")\n",
        "print(df_final.columns)\n",
        "\n",
        "Columns in dataset after adding grouped columns:\n",
        "Index(['cont2', 'cont12', 'cont14', 'cont11', 'cont9', 'cat7_encoded',\n",
        "       'cat57_encoded', 'cat89_grouped', 'cat101_grouped', 'cat102_grouped',\n",
        "       'cat105_grouped', 'cat107_grouped', 'cat113_grouped', 'cat115_grouped',\n",
        "       'cat116_grouped'],\n",
        "      dtype='object')\n",
        "\n",
        "# Step 6: Prepare the feature set and target variable\n",
        "X = df_final\n",
        "y = df['loss']\n",
        "\n",
        "\n",
        "# Step 7: Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Save the train-test split data\n",
        "joblib.dump((X_train, X_test, y_train, y_test), 'train_test_split.joblib')"
      ],
      "metadata": {
        "id": "YMHmwpJ3PC-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12taz3F0PfRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting"
      ],
      "metadata": {
        "id": "JuB_VGJKPf92"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cG8Ppxy7PsJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = joblib.load('/content/drive/MyDrive/train_test_split.joblib')\n"
      ],
      "metadata": {
        "id": "WuL4qGRTPr0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "                                \"n_estimators\":200,  # Number of boosting stages\n",
        "                                \"max_depth\":3,      # Depth of each tree\n",
        "                                  \"learning_rate\":0.01, # Learning rate for model updates\n",
        "          }\n",
        "\n",
        "\n",
        "model = GradientBoostingRegressor(**params)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "DQUGmtmbPnNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search = RandomizedSearchCV(model, param_distributions=params,\n",
        "                                   n_iter=10, scoring='r2', cv=5, random_state=42)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"R² Score: {r2}\")"
      ],
      "metadata": {
        "id": "jvi0ZO2aPtKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the plot with a timestamp in the filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
        "directory = r\"C:\\Users\\Maryl\\OneDrive\\Allstate AI\\Graphs\"\n",
        "version_folder = os.path.join(directory, f'Gradient_Boosting_Machine_{timestamp}')\n",
        "os.makedirs(version_folder)\n",
        "file_path= os.path.join(version_folder, '{}_Gradient_Boosting_Machine'.format(timestamp))\n",
        "\n",
        "# savetime = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
        "# #joblib.dump(youden_index, 'ML Prediction Results/{}_youden_index-v2.pkl'.format(savetime))\n",
        "# directory = 'ML Prediction Results'\n",
        "# version_folder = os.path.join(directory, f'SVM_Version_{savetime}')\n",
        "# os.makedirs(version_folder)\n",
        "# file_path = os.path.join(version_folder, '{}_youden_index-v2.pkl'.format(savetime))\n",
        "# joblib.dump(youden_index,file_path)\n",
        "\n",
        "\n",
        "# plt.savefig(f'predicted_vs_actual_{timestamp}.png')  # Save as PNG\n",
        "# #plot is not saving, figure out why\n",
        "\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.6, label='Predicted vs Actual')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction')  # Diagonal line\n",
        "plt.xlim([y_test.min(), y_test.max()])\n",
        "plt.ylim([y_test.min(), y_test.max()])\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title(f'Predicted vs Actual Values\\nR² Score: {r2:.2f}')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(os.path.join(version_folder, '{}_R2_graph.png'.format(timestamp)), transparent = False, dpi = 650, bbox_inches = \"tight\")\n"
      ],
      "metadata": {
        "id": "CafwZ-eaPxR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Scatter plot for True vs Predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, color='b')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(f\"True vs Predicted Values (RMSE = {rmse:.2f})\")\n",
        "plt.savefig(os.path.join(version_folder, '{}_RMSE_plot.png'.format(timestamp)),transparent = False, dpi = 650, bbox_inches = \"tight\")\n",
        "# plt.savefig(os.path.join(version_folder, '{}_predicted_vs_actual.png'.format(savetime)),\n",
        "# plt.savefig(f'predicted_vs_actual_{timestamp}.png', bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lgYoxQuVP4fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate residuals\n",
        "residual = y_pred - y_test\n",
        "print(f\"Residual Value (preicted value - the actal value): {residual}\")\n",
        "\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, residual, alpha=0.5, color='purple')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Residuals (Predicted - Actual)\")\n",
        "plt.title(\"Residuals of Predictions\")\n",
        "plt.savefig(os.path.join(version_folder, '{}_residuals_plot_'.format(timestamp)), transparent = False, dpi = 650, bbox_inches = \"tight\")\n",
        "# plt.savefig(f'residuals_plot_{timestamp}.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5By0blRvQEPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XG Boost Maryln"
      ],
      "metadata": {
        "id": "JqCjPx4SQBEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = joblib.load('/content/drive/MyDrive/train_test_split.joblib')\n",
        "# X_train, X_test, y_train, y_test = joblib.load('/train_test_split.joblib')\n",
        "\n",
        "\n",
        "import os\n",
        "file_path = '/train_test_split.joblib'\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    X_train, X_test, y_train, y_test = joblib.load(file_path)\n",
        "else:\n",
        "    print(\"File not found. Please check the file path.\")\n",
        ""
      ],
      "metadata": {
        "id": "JSAPFMmkQC0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)"
      ],
      "metadata": {
        "id": "K2NWK3obQPjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter grid\n",
        "param_dist = {\n",
        "    'n_estimators':  [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 9, 11, 13],\n",
        "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.15, 0.2],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 0.1, 0.3, 0.4, 0.5]\n",
        "}"
      ],
      "metadata": {
        "id": "sM6aBxkmQRdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomized search\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='r2', cv=5, random_state=42)\n",
        "\n",
        "\n",
        "random_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "OYkXMe1SQTPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Save the plot with a timestamp in the filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
        "directory = r\"C:\\Users\\Maryl\\OneDrive\\Allstate AI\\Graphs\"\n",
        "version_folder = os.path.join(directory, f'XGBoost_{timestamp}')\n",
        "os.makedirs(version_folder)\n",
        "file_path= os.path.join(version_folder, '{}_XGBoost'.format(timestamp))\n",
        "\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.6, label='Predicted vs Actual')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction')  # Diagonal line\n",
        "plt.xlim([y_test.min(), y_test.max()])\n",
        "plt.ylim([y_test.min(), y_test.max()])\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title(f'Predicted vs Actual Values\\nR² Score: {r2:.2f}')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(os.path.join(version_folder, '{}_R2_graph.png'.format(timestamp)), transparent = False, dpi = 650, bbox_inches = \"tight\")\n",
        "\n"
      ],
      "metadata": {
        "id": "v7OYIzVlQWsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Scatter plot for True vs Predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, color='b')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(f\"True vs Predicted Values (RMSE = {rmse:.2f})\")\n",
        "# plt.savefig(f'predicted_vs_actual_{timestamp}.png', bbox_inches='tight')\n",
        "plt.savefig(os.path.join(version_folder, '{}_RMSE_plot.png'.format(timestamp)),transparent = False, dpi = 650, bbox_inches = \"tight\")\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jf-joBb-Qc30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate residuals\n",
        "residual = y_pred - y_test\n",
        "print(f\"Residual Value (preicted value - the actal value): {residual}\")\n",
        "\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, residual, alpha=0.5, color='purple')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Residuals (Predicted - Actual)\")\n",
        "plt.title(\"Residuals of Predictions\")\n",
        "# plt.savefig(f'residuals_plot_{timestamp}.png', bbox_inches='tight')\n",
        "plt.savefig(os.path.join(version_folder, '{}_residuals_plot_'.format(timestamp)), transparent = False, dpi = 650, bbox_inches = \"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZRHPzpudQg1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xU3RZbhPQmb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression"
      ],
      "metadata": {
        "id": "6Nbrz_hbQnJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import category_encoders as ce\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "filename = \"claims_data.csv\"\n",
        "df = pd.read_csv(filename, header=0)\n",
        "df.head(20)\n",
        "id\tcat1\tcat2\tcat3\tcat4\tcat5\tcat6\tcat7\tcat8\tcat9\t...\tcont6\tcont7\tcont8\tcont9\tcont10\tcont11\tcont12\tcont13\tcont14\tloss\n",
        "0\t1\tA\tB\tA\tB\tA\tA\tA\tA\tB\t...\t0.718367\t0.335060\t0.30260\t0.67135\t0.83510\t0.569745\t0.594646\t0.822493\t0.714843\t2213.18\n",
        "1\t2\tA\tB\tA\tA\tA\tA\tA\tA\tB\t...\t0.438917\t0.436585\t0.60087\t0.35127\t0.43919\t0.338312\t0.366307\t0.611431\t0.304496\t1283.60\n",
        "2\t5\tA\tB\tA\tA\tB\tA\tA\tA\tB\t...\t0.289648\t0.315545\t0.27320\t0.26076\t0.32446\t0.381398\t0.373424\t0.195709\t0.774425\t3005.09\n",
        "3\t10\tB\tB\tA\tB\tA\tA\tA\tA\tB\t...\t0.440945\t0.391128\t0.31796\t0.32128\t0.44467\t0.327915\t0.321570\t0.605077\t0.602642\t939.85\n",
        "4\t11\tA\tB\tA\tB\tA\tA\tA\tA\tB\t...\t0.178193\t0.247408\t0.24564\t0.22089\t0.21230\t0.204687\t0.202213\t0.246011\t0.432606\t2763.85\n",
        "5\t13\tA\tB\tA\tA\tA\tA\tA\tA\tB\t...\t0.364464\t0.401162\t0.26847\t0.46226\t0.50556\t0.366788\t0.359249\t0.345247\t0.726792\t5142.87\n",
        "6\t14\tA\tA\tA\tA\tB\tA\tA\tA\tA\t...\t0.381515\t0.363768\t0.24564\t0.40455\t0.47225\t0.334828\t0.352251\t0.342239\t0.382931\t1132.22\n",
        "7\t20\tA\tB\tA\tB\tA\tA\tA\tA\tB\t...\t0.867021\t0.583389\t0.90267\t0.84847\t0.80218\t0.644013\t0.785706\t0.859764\t0.242416\t3585.75\n",
        "8\t23\tA\tB\tB\tB\tB\tA\tA\tA\tB\t...\t0.628534\t0.384099\t0.61229\t0.38249\t0.51111\t0.682315\t0.669033\t0.756454\t0.361191\t10280.20\n",
        "9\t24\tA\tB\tA\tA\tB\tB\tA\tA\tB\t...\t0.713343\t0.469223\t0.30260\t0.67135\t0.83510\t0.863052\t0.879347\t0.822493\t0.294523\t6184.59\n",
        "10\t25\tA\tB\tA\tA\tA\tA\tA\tA\tB\t...\t0.429383\t0.877905\t0.39455\t0.53565\t0.50556\t0.550529\t0.538473\t0.336261\t0.715009\t6396.85\n",
        "11\t33\tA\tB\tA\tA\tB\tA\tA\tA\tB\t...\t0.314683\t0.370419\t0.58354\t0.46226\t0.38016\t0.644013\t0.665644\t0.339244\t0.799124\t5965.73\n",
        "12\t34\tB\tA\tA\tA\tB\tA\tA\tA\tA\t...\t0.408772\t0.363312\t0.32843\t0.32128\t0.44467\t0.327915\t0.321570\t0.605077\t0.818358\t1193.05\n",
        "13\t41\tB\tA\tA\tA\tB\tB\tA\tA\tA\t...\t0.241574\t0.255339\t0.58934\t0.32496\t0.26029\t0.257148\t0.253044\t0.276878\t0.477578\t1071.77\n",
        "14\t47\tA\tA\tA\tA\tB\tA\tA\tA\tA\t...\t0.894903\t0.586433\t0.80058\t0.93383\t0.78770\t0.880469\t0.871011\t0.822493\t0.251278\t585.18\n",
        "15\t48\tA\tA\tA\tA\tB\tB\tA\tA\tA\t...\t0.570733\t0.547756\t0.80438\t0.44352\t0.63026\t0.385085\t0.377003\t0.516660\t0.340325\t1395.45\n",
        "16\t49\tA\tB\tB\tA\tA\tA\tA\tA\tB\t...\t0.411902\t0.593548\t0.31796\t0.38846\t0.48889\t0.457203\t0.447145\t0.301535\t0.205651\t6609.32\n",
        "17\t51\tA\tA\tA\tA\tA\tB\tA\tA\tA\t...\t0.688705\t0.437192\t0.67263\t0.83505\t0.59334\t0.678924\t0.665644\t0.684242\t0.407411\t2658.70\n",
        "18\t52\tA\tA\tB\tA\tA\tB\tA\tA\tA\t...\t0.443265\t0.637086\t0.36636\t0.52938\t0.39068\t0.678924\t0.665644\t0.304350\t0.310796\t4167.32\n",
        "19\t55\tA\tA\tA\tB\tA\tA\tA\tA\tA\t...\t0.436312\t0.544355\t0.48864\t0.36285\t0.20496\t0.388786\t0.406090\t0.648701\t0.830931\t3797.89\n",
        "20 rows × 132 columns\n",
        "\n",
        "# UNIVARIATE PLOTTING OF LOSS CATEGORY\n",
        "sns.histplot(data=df, x=\"loss\")\n",
        "<Axes: xlabel='loss', ylabel='Count'>\n",
        "\n",
        "df.columns\n",
        "Index(['id', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n",
        "       'cat9',\n",
        "       ...\n",
        "       'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n",
        "       'cont13', 'cont14', 'loss'],\n",
        "      dtype='object', length=132)\n",
        "# NO MISSING VALUES!\n",
        "df.isnull().sum()\n",
        "id        0\n",
        "cat1      0\n",
        "cat2      0\n",
        "cat3      0\n",
        "cat4      0\n",
        "         ..\n",
        "cont11    0\n",
        "cont12    0\n",
        "cont13    0\n",
        "cont14    0\n",
        "loss      0\n",
        "Length: 132, dtype: int64\n",
        "df.dtypes\n",
        "id          int64\n",
        "cat1       object\n",
        "cat2       object\n",
        "cat3       object\n",
        "cat4       object\n",
        "           ...\n",
        "cont11    float64\n",
        "cont12    float64\n",
        "cont13    float64\n",
        "cont14    float64\n",
        "loss      float64\n",
        "Length: 132, dtype: object\n",
        "to_encode = list(df.select_dtypes(include=['object']).columns)\n",
        "to_encode\n",
        "['cat1',\n",
        " 'cat2',\n",
        " 'cat3',\n",
        " 'cat4',\n",
        " 'cat5',\n",
        " 'cat6',\n",
        " 'cat7',\n",
        " 'cat8',\n",
        " 'cat9',\n",
        " 'cat10',\n",
        " 'cat11',\n",
        " 'cat12',\n",
        " 'cat13',\n",
        " 'cat14',\n",
        " 'cat15',\n",
        " 'cat16',\n",
        " 'cat17',\n",
        " 'cat18',\n",
        " 'cat19',\n",
        " 'cat20',\n",
        " 'cat21',\n",
        " 'cat22',\n",
        " 'cat23',\n",
        " 'cat24',\n",
        " 'cat25',\n",
        " 'cat26',\n",
        " 'cat27',\n",
        " 'cat28',\n",
        " 'cat29',\n",
        " 'cat30',\n",
        " 'cat31',\n",
        " 'cat32',\n",
        " 'cat33',\n",
        " 'cat34',\n",
        " 'cat35',\n",
        " 'cat36',\n",
        " 'cat37',\n",
        " 'cat38',\n",
        " 'cat39',\n",
        " 'cat40',\n",
        " 'cat41',\n",
        " 'cat42',\n",
        " 'cat43',\n",
        " 'cat44',\n",
        " 'cat45',\n",
        " 'cat46',\n",
        " 'cat47',\n",
        " 'cat48',\n",
        " 'cat49',\n",
        " 'cat50',\n",
        " 'cat51',\n",
        " 'cat52',\n",
        " 'cat53',\n",
        " 'cat54',\n",
        " 'cat55',\n",
        " 'cat56',\n",
        " 'cat57',\n",
        " 'cat58',\n",
        " 'cat59',\n",
        " 'cat60',\n",
        " 'cat61',\n",
        " 'cat62',\n",
        " 'cat63',\n",
        " 'cat64',\n",
        " 'cat65',\n",
        " 'cat66',\n",
        " 'cat67',\n",
        " 'cat68',\n",
        " 'cat69',\n",
        " 'cat70',\n",
        " 'cat71',\n",
        " 'cat72',\n",
        " 'cat73',\n",
        " 'cat74',\n",
        " 'cat75',\n",
        " 'cat76',\n",
        " 'cat77',\n",
        " 'cat78',\n",
        " 'cat79',\n",
        " 'cat80',\n",
        " 'cat81',\n",
        " 'cat82',\n",
        " 'cat83',\n",
        " 'cat84',\n",
        " 'cat85',\n",
        " 'cat86',\n",
        " 'cat87',\n",
        " 'cat88',\n",
        " 'cat89',\n",
        " 'cat90',\n",
        " 'cat91',\n",
        " 'cat92',\n",
        " 'cat93',\n",
        " 'cat94',\n",
        " 'cat95',\n",
        " 'cat96',\n",
        " 'cat97',\n",
        " 'cat98',\n",
        " 'cat99',\n",
        " 'cat100',\n",
        " 'cat101',\n",
        " 'cat102',\n",
        " 'cat103',\n",
        " 'cat104',\n",
        " 'cat105',\n",
        " 'cat106',\n",
        " 'cat107',\n",
        " 'cat108',\n",
        " 'cat109',\n",
        " 'cat110',\n",
        " 'cat111',\n",
        " 'cat112',\n",
        " 'cat113',\n",
        " 'cat114',\n",
        " 'cat115',\n",
        " 'cat116']\n",
        "# Why do some categories have more than 2 possible values? What do the values stand for?\n",
        "df[to_encode].nunique()\n",
        "cat1        2\n",
        "cat2        2\n",
        "cat3        2\n",
        "cat4        2\n",
        "cat5        2\n",
        "         ...\n",
        "cat112     51\n",
        "cat113     61\n",
        "cat114     19\n",
        "cat115     23\n",
        "cat116    326\n",
        "Length: 116, dtype: int64\n",
        "top_10 = list(df['cat112'].value_counts().head(60).index)\n",
        "\n",
        "top_10\n",
        "['E',\n",
        " 'AH',\n",
        " 'AS',\n",
        " 'J',\n",
        " 'AF',\n",
        " 'AN',\n",
        " 'N',\n",
        " 'U',\n",
        " 'AV',\n",
        " 'AK',\n",
        " 'K',\n",
        " 'AI',\n",
        " 'S',\n",
        " 'AP',\n",
        " 'G',\n",
        " 'F',\n",
        " 'AW',\n",
        " 'A',\n",
        " 'AR',\n",
        " 'C',\n",
        " 'O',\n",
        " 'D',\n",
        " 'AD',\n",
        " 'AY',\n",
        " 'Y',\n",
        " 'AG',\n",
        " 'AT',\n",
        " 'AA',\n",
        " 'AM',\n",
        " 'AL',\n",
        " 'R',\n",
        " 'AX',\n",
        " 'I',\n",
        " 'X',\n",
        " 'AE',\n",
        " 'Q',\n",
        " 'V',\n",
        " 'H',\n",
        " 'AO',\n",
        " 'T',\n",
        " 'L',\n",
        " 'W',\n",
        " 'AC',\n",
        " 'M',\n",
        " 'AU',\n",
        " 'B',\n",
        " 'P',\n",
        " 'AB',\n",
        " 'BA',\n",
        " 'AJ',\n",
        " 'AQ']\n",
        "\n",
        "categorical_columns = df.select_dtypes(include = ['object']).columns\n",
        "\n",
        "catlist = categorical_columns.tolist()\n",
        "\n",
        "#encoded_columns = pd.DataFrame()\n",
        "\n",
        "for col in catlist:\n",
        "    m_estimate_encoder = ce.MEstimateEncoder(cols=[col], m=5)\n",
        "    df[col] = m_estimate_encoder.fit_transform(df[col], df['loss'])\n",
        "    #encoded_columns = pd.concat([encoded_columns, encoded_col], axis=1)\n",
        "\n",
        "#df = df.drop(columns = categorical_columns)\n",
        "\n",
        "#df = pd.concat([df, encoded_columns], axis=1)\n",
        "\n",
        "#df= df_encoded.astype(float)\n",
        "\n",
        "df.head(20)\n",
        "id\tcat1\tcat2\tcat3\tcat4\tcat5\tcat6\tcat7\tcat8\tcat9\t...\tcont6\tcont7\tcont8\tcont9\tcont10\tcont11\tcont12\tcont13\tcont14\tloss\n",
        "0\t1\t3408.078419\t3800.057434\t2902.223547\t3488.354592\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.718367\t0.335060\t0.30260\t0.67135\t0.83510\t0.569745\t0.594646\t0.822493\t0.714843\t2213.18\n",
        "1\t2\t3408.078419\t3800.057434\t2902.223547\t2826.835029\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.438917\t0.436585\t0.60087\t0.35127\t0.43919\t0.338312\t0.366307\t0.611431\t0.304496\t1283.60\n",
        "2\t5\t3408.078419\t3800.057434\t2902.223547\t2826.835029\t3463.976775\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.289648\t0.315545\t0.27320\t0.26076\t0.32446\t0.381398\t0.373424\t0.195709\t0.774425\t3005.09\n",
        "3\t10\t1915.318476\t3800.057434\t2902.223547\t3488.354592\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.440945\t0.391128\t0.31796\t0.32128\t0.44467\t0.327915\t0.321570\t0.605077\t0.602642\t939.85\n",
        "4\t11\t3408.078419\t3800.057434\t2902.223547\t3488.354592\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.178193\t0.247408\t0.24564\t0.22089\t0.21230\t0.204687\t0.202213\t0.246011\t0.432606\t2763.85\n",
        "5\t13\t3408.078419\t3800.057434\t2902.223547\t2826.835029\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.364464\t0.401162\t0.26847\t0.46226\t0.50556\t0.366788\t0.359249\t0.345247\t0.726792\t5142.87\n",
        "6\t14\t3408.078419\t2454.167166\t2902.223547\t2826.835029\t3463.976775\t3259.907946\t2908.947835\t2975.560301\t2512.032345\t...\t0.381515\t0.363768\t0.24564\t0.40455\t0.47225\t0.334828\t0.352251\t0.342239\t0.382931\t1132.22\n",
        "7\t20\t3408.078419\t3800.057434\t2902.223547\t3488.354592\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.867021\t0.583389\t0.90267\t0.84847\t0.80218\t0.644013\t0.785706\t0.859764\t0.242416\t3585.75\n",
        "8\t23\t3408.078419\t3800.057434\t5365.512568\t3488.354592\t3463.976775\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.628534\t0.384099\t0.61229\t0.38249\t0.51111\t0.682315\t0.669033\t0.756454\t0.361191\t10280.20\n",
        "9\t24\t3408.078419\t3800.057434\t2902.223547\t2826.835029\t3463.976775\t2519.731150\t2908.947835\t2975.560301\t3827.569429\t...\t0.713343\t0.469223\t0.30260\t0.67135\t0.83510\t0.863052\t0.879347\t0.822493\t0.294523\t6184.59\n",
        "10\t25\t3408.078419\t3800.057434\t2902.223547\t2826.835029\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.429383\t0.877905\t0.39455\t0.53565\t0.50556\t0.550529\t0.538473\t0.336261\t0.715009\t6396.85\n",
        "11\t33\t3408.078419\t3800.057434\t2902.223547\t2826.835029\t3463.976775\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.314683\t0.370419\t0.58354\t0.46226\t0.38016\t0.644013\t0.665644\t0.339244\t0.799124\t5965.73\n",
        "12\t34\t1915.318476\t2454.167166\t2902.223547\t2826.835029\t3463.976775\t3259.907946\t2908.947835\t2975.560301\t2512.032345\t...\t0.408772\t0.363312\t0.32843\t0.32128\t0.44467\t0.327915\t0.321570\t0.605077\t0.818358\t1193.05\n",
        "13\t41\t1915.318476\t2454.167166\t2902.223547\t2826.835029\t3463.976775\t2519.731150\t2908.947835\t2975.560301\t2512.032345\t...\t0.241574\t0.255339\t0.58934\t0.32496\t0.26029\t0.257148\t0.253044\t0.276878\t0.477578\t1071.77\n",
        "14\t47\t3408.078419\t2454.167166\t2902.223547\t2826.835029\t3463.976775\t3259.907946\t2908.947835\t2975.560301\t2512.032345\t...\t0.894903\t0.586433\t0.80058\t0.93383\t0.78770\t0.880469\t0.871011\t0.822493\t0.251278\t585.18\n",
        "15\t48\t3408.078419\t2454.167166\t2902.223547\t2826.835029\t3463.976775\t2519.731150\t2908.947835\t2975.560301\t2512.032345\t...\t0.570733\t0.547756\t0.80438\t0.44352\t0.63026\t0.385085\t0.377003\t0.516660\t0.340325\t1395.45\n",
        "16\t49\t3408.078419\t3800.057434\t5365.512568\t2826.835029\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t3827.569429\t...\t0.411902\t0.593548\t0.31796\t0.38846\t0.48889\t0.457203\t0.447145\t0.301535\t0.205651\t6609.32\n",
        "17\t51\t3408.078419\t2454.167166\t2902.223547\t2826.835029\t2814.657333\t2519.731150\t2908.947835\t2975.560301\t2512.032345\t...\t0.688705\t0.437192\t0.67263\t0.83505\t0.59334\t0.678924\t0.665644\t0.684242\t0.407411\t2658.70\n",
        "18\t52\t3408.078419\t2454.167166\t5365.512568\t2826.835029\t2814.657333\t2519.731150\t2908.947835\t2975.560301\t2512.032345\t...\t0.443265\t0.637086\t0.36636\t0.52938\t0.39068\t0.678924\t0.665644\t0.304350\t0.310796\t4167.32\n",
        "19\t55\t3408.078419\t2454.167166\t2902.223547\t3488.354592\t2814.657333\t3259.907946\t2908.947835\t2975.560301\t2512.032345\t...\t0.436312\t0.544355\t0.48864\t0.36285\t0.20496\t0.388786\t0.406090\t0.648701\t0.830931\t3797.89\n",
        "20 rows × 132 columns\n",
        "\n",
        "top_10 = list(df['cat112'].value_counts().head(60).index)\n",
        "\n",
        "top_10\n",
        "[3208.3752740598875,\n",
        " 3476.4887378474764,\n",
        " 3409.8779698103626,\n",
        " 2832.8601330146266,\n",
        " 3426.6158314764057,\n",
        " 3111.3939186731213,\n",
        " 3043.5809350234513,\n",
        " 2594.8842074426925,\n",
        " 2381.311463228336,\n",
        " 2637.109041513646,\n",
        " 3277.033962471694,\n",
        " 2379.012328234823,\n",
        " 3119.0660623938065,\n",
        " 2302.829178134419,\n",
        " 3249.8959780738573,\n",
        " 2949.554359045133,\n",
        " 2856.3516598185233,\n",
        " 2803.8196640845817,\n",
        " 2785.3912398431853,\n",
        " 3249.903575786185,\n",
        " 2728.5320833767596,\n",
        " 2859.0727626838484,\n",
        " 3291.3359950705403,\n",
        " 2788.348110238442,\n",
        " 2837.771016540081,\n",
        " 2964.038546727807,\n",
        " 3056.9099831075564,\n",
        " 2788.0881127033304,\n",
        " 2677.82482419434,\n",
        " 3185.2353113906165,\n",
        " 2919.536009244991,\n",
        " 2943.187208923401,\n",
        " 2473.651797278677,\n",
        " 2817.8997294928495,\n",
        " 2431.9903199384385,\n",
        " 2844.9589955242477,\n",
        " 2970.4514733930514,\n",
        " 2854.65138956302,\n",
        " 3214.006787436642,\n",
        " 2566.558818304848,\n",
        " 3060.2343542737954,\n",
        " 3347.4481296745707,\n",
        " 2770.9505412382355,\n",
        " 2819.0300865503377,\n",
        " 2577.761249267312,\n",
        " 3190.7460710942755,\n",
        " 2933.419363572628,\n",
        " 3229.488081387848,\n",
        " 3122.3346073248717,\n",
        " 3186.3073048882543,\n",
        " 2778.3228122385694]\n",
        "corr_matrix = df.corr()\n",
        "plt.figure(figsize=(20, 15))  # Width x Height in inches\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', annot_kws={\"size\": 10}, cbar_kws={'label': 'Correlation'})\n",
        "plt.show()\n",
        "\n",
        "label_column = 'loss'\n",
        "\n",
        "cont_feats =  ['cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n",
        "       'cont13', 'cont14']\n",
        "\n",
        "cat_feats= ['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n",
        "       'cat9']\n",
        "\n",
        "feats = cont_feats + cat_feats\n",
        "\n",
        "#df_encoded = pd.get_dummies(df[cat_feats])\n",
        "#df_numeric = df[cont_feats]\n",
        "df_combined = pd.concat((df[cat_feats], df[cont_feats]), axis=1)\n",
        "\n",
        "df_combined[label_column] = df[label_column]\n",
        "\n",
        "correlation_matrix = df_combined.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix[[label_column]].sort_values(by=label_column, ascending=False), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix for Loss Label')\n",
        "plt.show()\n",
        "\n",
        "# TRAINED LINEAR REGRESSION MODEL BASED ON THE TARGET ENCODED DATA\n",
        "features = ['cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116']\n",
        "\n",
        "X = df[features]\n",
        "y = df['loss']\n",
        "\n",
        "print(X)\n",
        "             cat109       cat110       cat111       cat112       cat113  \\\n",
        "0       2639.853403  2555.547630  3112.781574  3409.877970  2745.118673\n",
        "1       3146.281369  3053.180857  2826.835029  2381.311463  2874.502783\n",
        "2       2609.825032  2938.455639  2826.835029  3249.903576  2942.939477\n",
        "3       3146.281369  3024.901680  3112.781574  3043.580935  2907.769445\n",
        "4       2058.911903  2427.051291  3112.781574  2837.771017  2874.502783\n",
        "...             ...          ...          ...          ...          ...\n",
        "188313  3146.281369  3047.405943  2826.835029  3249.895978  2874.502783\n",
        "188314  3146.281369  3180.011145  2826.835029  2381.311463  2601.824868\n",
        "188315  3146.281369  3076.768862  2826.835029  3347.448130  2942.939477\n",
        "188316  3146.281369  2786.703811  2826.835029  3409.877970  2923.442501\n",
        "188317  3146.281369  3047.405943  5167.734712  3409.877970  3065.474265\n",
        "\n",
        "             cat114       cat115       cat116\n",
        "0       3259.907946  2948.542000  2917.710468\n",
        "1       3259.907946  2948.542000  3107.659307\n",
        "2       3259.907946  2991.051898  2696.415612\n",
        "3       3259.907946  2948.542000  2911.931661\n",
        "4       3259.907946  3016.759217  3037.328951\n",
        "...             ...          ...          ...\n",
        "188313  3259.907946  3054.090013  3037.328951\n",
        "188314  2088.702603  3140.728386  2805.428945\n",
        "188315  3259.907946  3016.759217  2911.931661\n",
        "188316  3259.907946  2948.542000  3162.497643\n",
        "188317  3259.907946  2948.542000  3798.173581\n",
        "\n",
        "[188318 rows x 8 columns]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "# Create the  LinearRegression model object\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions on the test data\n",
        "prediction = model.predict(X_test)\n",
        "print('Model Summary:\\n')\n",
        "\n",
        "# Print intercept (alpha)\n",
        "print('Intercept:')\n",
        "print('alpha = ' , model.intercept_)\n",
        "\n",
        "# Print weights\n",
        "print('\\nWeights:')\n",
        "i = 0\n",
        "for w in model.coef_:\n",
        "    print('w_',i+1,'= ', w, ' [ weight of ', features[i],']')\n",
        "    i += 1\n",
        "Model Summary:\n",
        "\n",
        "Intercept:\n",
        "alpha =  -13616.105827116236\n",
        "\n",
        "Weights:\n",
        "w_ 1 =  0.4128165922689112  [ weight of  cat109 ]\n",
        "w_ 2 =  0.2934223773862843  [ weight of  cat110 ]\n",
        "w_ 3 =  0.9170956098793612  [ weight of  cat111 ]\n",
        "w_ 4 =  0.8080673235397873  [ weight of  cat112 ]\n",
        "w_ 5 =  0.5688995338963799  [ weight of  cat113 ]\n",
        "w_ 6 =  0.9612427473241598  [ weight of  cat114 ]\n",
        "w_ 7 =  0.83765695235362  [ weight of  cat115 ]\n",
        "w_ 8 =  0.6851911572023519  [ weight of  cat116 ]\n",
        "Looking at the data, cat111, cat112, cat114, and cat115 has the highest weights from cat 109-116.\n",
        "# Print mean squared error\n",
        "print('\\nModel Performance\\n\\nRMSE =   %.2f'\n",
        "      % np.sqrt(mean_squared_error(y_test, prediction)))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(' R^2 =   %.2f'\n",
        "      % r2_score(y_test, prediction))\n",
        "Model Performance\n",
        "\n",
        "RMSE =   2702.81\n",
        " R^2 =   0.12\n",
        "Our RMSE score is very high, meaning that the predicted values and real values have a significant difference. R^2 is also very low, suggesting that there\n",
        "is no real value between the features and label.\n",
        "\n",
        "#TRAINING LINEAR REGRESSION MODEL ON HIGHEST WEIGHTED CATEGORY FROM BEFORE: CAT114\n",
        "X = df['cat114'].to_frame()\n",
        "y = df['loss']\n",
        "\n",
        "print(X)\n",
        "             cat114\n",
        "0       3259.907946\n",
        "1       3259.907946\n",
        "2       3259.907946\n",
        "3       3259.907946\n",
        "4       3259.907946\n",
        "...             ...\n",
        "188313  3259.907946\n",
        "188314  2088.702603\n",
        "188315  3259.907946\n",
        "188316  3259.907946\n",
        "188317  3259.907946\n",
        "\n",
        "[188318 rows x 1 columns]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "# Create the  LinearRegression model object\n",
        "model2 = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions on the test data\n",
        "prediction2 = model2.predict(X_test)\n",
        "# Weight_1 (weight of feature LogGDP)\n",
        "print('Model Summary\\n\\nWeight_1 =  ', model2.coef_[0])\n",
        "# alpha\n",
        "print('Alpha = ', model2.intercept_, '[ intercept ]')\n",
        "Model Summary\n",
        "\n",
        "Weight_1 =   1.0154671486187499 [ weight of feature LogGDP ]\n",
        "Alpha =  -45.11987339779034 [ intercept ]\n",
        "#The mean squared error\n",
        "print('\\nModel Performance\\n\\nRMSE =   %.2f'\n",
        "      % np.sqrt(mean_squared_error(y_test, prediction2)))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(' R^2 =   %.2f'\n",
        "      % r2_score(y_test, prediction2))\n",
        "Model Performance\n",
        "\n",
        "RMSE =   2829.83\n",
        " R^2 =   0.03\n",
        "R^2 decreased even more when trained with only cat114 as the feature.\n",
        "\n",
        "\n",
        "# TRAINING MODEL USING ONLY THE COLUMNS THAT HAVE A OR B\n",
        "cols_with_A_or_B = df.columns[df.apply(lambda col: col.isin(['A', 'B']).all())]\n",
        "\n",
        "# Create a new DataFrame with only the extracted columns\n",
        "df_AB = df[cols_with_A_or_B]\n",
        "df_AB = df_AB.replace({'A': 1, 'B': 0})\n",
        "\n",
        "print(df_AB)\n",
        "C:\\Users\\shimr\\AppData\\Local\\Temp\\ipykernel_5864\\1011918628.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
        "  df_AB = df_AB.replace({'A': 1, 'B': 0})\n",
        "        cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10  ...  \\\n",
        "0          1     0     1     0     1     1     1     1     0      1  ...\n",
        "1          1     0     1     1     1     1     1     1     0      0  ...\n",
        "2          1     0     1     1     0     1     1     1     0      0  ...\n",
        "3          0     0     1     0     1     1     1     1     0      1  ...\n",
        "4          1     0     1     0     1     1     1     1     0      0  ...\n",
        "...      ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...\n",
        "188313     1     0     1     1     1     1     1     1     0      1  ...\n",
        "188314     1     1     1     1     1     0     1     1     1      1  ...\n",
        "188315     1     0     1     1     1     1     1     0     0      1  ...\n",
        "188316     1     0     1     1     1     1     1     1     0      0  ...\n",
        "188317     0     1     1     0     1     1     1     1     1      1  ...\n",
        "\n",
        "        cat63  cat64  cat65  cat66  cat67  cat68  cat69  cat70  cat71  cat72\n",
        "0           1      1      1      1      1      1      1      1      1      1\n",
        "1           1      1      1      1      1      1      1      1      1      1\n",
        "2           1      1      1      1      1      1      1      1      1      1\n",
        "3           1      1      1      1      1      1      1      1      1      1\n",
        "4           1      1      1      1      1      1      1      1      1      0\n",
        "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...\n",
        "188313      1      1      1      1      1      1      1      1      1      0\n",
        "188314      1      1      1      1      1      1      1      1      1      1\n",
        "188315      1      1      1      0      1      1      1      1      1      1\n",
        "188316      1      1      1      1      1      1      1      1      1      1\n",
        "188317      1      1      1      1      1      1      1      1      1      0\n",
        "\n",
        "[188318 rows x 72 columns]\n",
        "df_AB['loss'] = df['loss']\n",
        "#df_ABfeatures = df_AB.columns\n",
        "X = df_AB.columns\n",
        "y = df['loss']\n",
        "\n",
        "print(X)\n",
        "Index(['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9',\n",
        "       'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17',\n",
        "       'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25',\n",
        "       'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33',\n",
        "       'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41',\n",
        "       'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49',\n",
        "       'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57',\n",
        "       'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65',\n",
        "       'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'loss'],\n",
        "      dtype='object')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "Cell In[62], line 1\n",
        "----> 1 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\n",
        "    207 try:\n",
        "    208     with config_context(\n",
        "    209         skip_parameter_validation=(\n",
        "    210             prefer_skip_nested_validation or global_skip_validation\n",
        "    211         )\n",
        "    212     ):\n",
        "--> 213         return func(*args, **kwargs)\n",
        "    214 except InvalidParameterError as e:\n",
        "    215     # When the function is just a wrapper around an estimator, we allow\n",
        "    216     # the function to delegate validation to the estimator, but we replace\n",
        "    217     # the name of the estimator by the name of the function in the error\n",
        "    218     # message to avoid confusion.\n",
        "    219     msg = re.sub(\n",
        "    220         r\"parameter of \\w+ must be\",\n",
        "    221         f\"parameter of {func.__qualname__} must be\",\n",
        "    222         str(e),\n",
        "    223     )\n",
        "\n",
        "File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:2777, in train_test_split(test_size, train_size, random_state, shuffle, stratify, *arrays)\n",
        "   2774 if n_arrays == 0:\n",
        "   2775     raise ValueError(\"At least one array required as input\")\n",
        "-> 2777 arrays = indexable(*arrays)\n",
        "   2779 n_samples = _num_samples(arrays[0])\n",
        "   2780 n_train, n_test = _validate_shuffle_split(\n",
        "   2781     n_samples, test_size, train_size, default_test_size=0.25\n",
        "   2782 )\n",
        "\n",
        "File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:514, in indexable(*iterables)\n",
        "    484 \"\"\"Make arrays indexable for cross-validation.\n",
        "    485\n",
        "    486 Checks consistent length, passes through None, and ensures that everything\n",
        "   (...)\n",
        "    510 [[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\n",
        "    511 \"\"\"\n",
        "    513 result = [_make_indexable(X) for X in iterables]\n",
        "--> 514 check_consistent_length(*result)\n",
        "    515 return result\n",
        "\n",
        "File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:457, in check_consistent_length(*arrays)\n",
        "    455 uniques = np.unique(lengths)\n",
        "    456 if len(uniques) > 1:\n",
        "--> 457     raise ValueError(\n",
        "    458         \"Found input variables with inconsistent numbers of samples: %r\"\n",
        "    459         % [int(l) for l in lengths]\n",
        "    460     )\n",
        "\n",
        "ValueError: Found input variables with inconsistent numbers of samples: [73, 188318]\n",
        ""
      ],
      "metadata": {
        "id": "xA5Tw9U0Qow5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Td2BYdu0Q_0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bode Final Model"
      ],
      "metadata": {
        "id": "pG00gu2PRBvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Data Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "#XGBoost Libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "#Error Metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "#HyperParameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Perform K-Fold Cross-Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#Random Distribution\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "#Importance Plot\n",
        "from xgboost import plot_importance\n",
        "df = pd.read_csv(\"/Users/bodechiu/Desktop/Break Through Tech AI/AI Studio Project/claims_data.csv\")\n",
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "corrMatrix = df_numeric.corr()\n",
        "plt.figure(figsize=(20, 15))  # Width x Height in inches\n",
        "sns.heatmap(corrMatrix, annot=True, fmt='.2f', cmap='coolwarm', annot_kws={\"size\": 10}, cbar_kws={'label': 'Correlation'})\n",
        "plt.show()\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "categorical_columns = [f'cat{i}' for i in range(1, 117)]\n",
        "continuous_columns = [f'cont{i}' for i in range(1, 15)]\n",
        "target_column = 'loss'\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "encoded_categorical = encoder.fit_transform(df[categorical_columns])\n",
        "\n",
        "# Get the feature names from the encoder and organize them\n",
        "# The feature names generated will include the original column and category\n",
        "categories = encoder.categories_\n",
        "organized_feature_names = []\n",
        "\n",
        "for col_index, col_name in enumerate(categorical_columns):\n",
        "    for cat_index, category in enumerate(categories[col_index][1:]):  # Skip the first category due to 'drop=first'\n",
        "        organized_feature_names.append(f'{col_name}_{category}')\n",
        "\n",
        "# Convert the encoded categorical data to a DataFrame with the organized feature names\n",
        "encoded_df = pd.DataFrame(encoded_categorical, columns=organized_feature_names)\n",
        "\n",
        "# Combine the continuous columns and the one-hot encoded categorical columns\n",
        "X = pd.concat([encoded_df, df[continuous_columns]], axis=1)\n",
        "y = df[target_column]\n",
        "\n",
        "\n",
        "# Step 1: Create a new feature with uniform distribution\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X['uniform_feature'] = np.random.uniform(0, 1, X.shape[0])  # Add a uniform feature\n",
        "\n",
        "# Step 2: Split the data into training and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the model with the new feature\n",
        "best_params = {\n",
        "    'colsample_bytree': 0.8998609717152555,\n",
        "    'learning_rate': 0.019333132642723086,\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 882,\n",
        "    'subsample': 0.9560699842170359,\n",
        "    'eval_metric': 'rmse'  # Add evaluation metric for consistency\n",
        "}\n",
        "model = xgb.XGBRegressor(**best_params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_importance(model, max_num_features=20, importance_type=\"weight\")  # Adjust max_num_features as needed\n",
        "plt.title(\"XGBoost Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "# Extract feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "features = X.columns\n",
        "\n",
        "# Identify threshold using the importance of the uniform feature\n",
        "uniform_feature_importance = importances[features.get_loc('uniform_feature')]\n",
        "\n",
        "# Step 5: Drop all features that fall below the value for this new feature\n",
        "important_features = features[importances >= uniform_feature_importance]\n",
        "X_train_filtered = X_train[important_features]\n",
        "X_test_filtered = X_test[important_features]\n",
        "\n",
        "# Retrain the model with filtered features\n",
        "filtered_model = xgb.XGBRegressor(**best_params)\n",
        "filtered_model.fit(X_train_filtered, y_train)\n",
        "\n",
        "# Step 6: Make predictions using the model with filtered features\n",
        "y_pred_filtered = filtered_model.predict(X_test_filtered)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R²\n",
        "mse_filtered = mean_squared_error(y_test, y_pred_filtered)\n",
        "rmse_filtered = np.sqrt(mse_filtered)\n",
        "r2_filtered = r2_score(y_test, y_pred_filtered)\n",
        "\n",
        "# Calculate the variance of the target variable in the test set\n",
        "variance = y_test.var()\n",
        "\n",
        "# Normalize the MSE by dividing it by the variance of the target variable\n",
        "normalized_mse = mse_filtered / variance\n",
        "\n",
        "print(f\"Mean Squared Error (Filtered): {mse_filtered}\")\n",
        "print(f\"Normalized Mean Squared Error: {normalized_mse}\")\n",
        "print(f\"Root Mean Squared Error (Filtered): {rmse_filtered}\")\n",
        "print(f\"R² (Filtered): {r2_filtered}\")\n",
        "\n",
        "\n",
        "# Step 7: Look at data where the prediction is \"good\"\n",
        "residuals = np.abs(y_test - y_pred_filtered)\n",
        "threshold = residuals.mean()  # You could set a threshold based on the mean of residuals\n",
        "\n",
        "# Display data points where residuals are below the threshold (good predictions)\n",
        "good_predictions = X_test_filtered[residuals < threshold]\n",
        "print(\"Data points with good predictions:\")\n",
        "print(good_predictions)\n",
        "\n",
        "# Step 8: Look at top predictors\n",
        "top_predictors = important_features[np.argsort(importances[importances >= uniform_feature_importance])[::-1]]\n",
        "print(\"Top predictors based on filtered model:\")\n",
        "print(top_predictors)\n",
        "<Figure size 1000x800 with 0 Axes>\n",
        "\n",
        "Mean Squared Error (Filtered): 3432087.742558672\n",
        "Normalized Mean Squared Error: 0.4206084507157916\n",
        "Root Mean Squared Error (Filtered): 1852.5894695152167\n",
        "R² (Filtered): 0.5793803816010521\n",
        "Data points with good predictions:\n",
        "        cat1_B  cat2_B  cat4_B  cat5_B  cat6_B  cat7_B  cat9_B  cat10_B  \\\n",
        "10168      0.0     1.0     1.0     0.0     0.0     0.0     1.0      0.0\n",
        "6936       0.0     0.0     0.0     0.0     1.0     0.0     0.0      0.0\n",
        "117312     1.0     1.0     0.0     0.0     0.0     0.0     1.0      0.0\n",
        "70551      0.0     0.0     0.0     1.0     0.0     0.0     0.0      0.0\n",
        "97059      0.0     0.0     1.0     0.0     0.0     0.0     0.0      0.0\n",
        "...        ...     ...     ...     ...     ...     ...     ...      ...\n",
        "88497      1.0     0.0     0.0     1.0     0.0     0.0     0.0      0.0\n",
        "56217      0.0     1.0     1.0     0.0     0.0     0.0     1.0      0.0\n",
        "162470     0.0     1.0     1.0     0.0     0.0     0.0     1.0      1.0\n",
        "114899     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0\n",
        "35726      1.0     0.0     0.0     0.0     1.0     0.0     0.0      0.0\n",
        "\n",
        "        cat11_B  cat12_B  ...     cont6     cont7    cont8    cont9   cont10  \\\n",
        "10168       0.0      0.0  ...  0.817706  0.480176  0.90055  0.84847  0.80218\n",
        "6936        0.0      0.0  ...  0.687443  0.499166  0.54236  0.50420  0.51111\n",
        "117312      0.0      0.0  ...  0.394921  0.374803  0.24564  0.46853  0.50556\n",
        "70551       0.0      0.0  ...  0.446460  0.459649  0.69840  0.39447  0.46119\n",
        "97059       0.0      0.0  ...  0.344288  0.354907  0.95332  0.33051  0.31480\n",
        "...         ...      ...  ...       ...       ...      ...      ...      ...\n",
        "88497       0.0      0.0  ...  0.439206  0.415949  0.45883  0.46853  0.52221\n",
        "56217       0.0      0.0  ...  0.925649  0.604335  0.96843  0.93383  0.83510\n",
        "162470      1.0      1.0  ...  0.303881  0.356898  0.36083  0.36091  0.36458\n",
        "114899      0.0      0.0  ...  0.419901  0.416308  0.72775  0.37458  0.40666\n",
        "35726       0.0      0.0  ...  0.384572  0.296392  0.92347  0.48530  0.29134\n",
        "\n",
        "          cont11    cont12    cont13    cont14  uniform_feature\n",
        "10168   0.550529  0.590961  0.814573  0.721577         0.498151\n",
        "6936    0.698978  0.685713  0.695650  0.721610         0.317259\n",
        "117312  0.453334  0.443374  0.263731  0.757043         0.327384\n",
        "70551   0.307628  0.305148  0.660756  0.820556         0.736106\n",
        "97059   0.291268  0.286079  0.486670  0.411261         0.697651\n",
        "...          ...       ...       ...       ...              ...\n",
        "88497   0.441763  0.443374  0.324464  0.833359         0.017276\n",
        "56217   0.826362  0.891717  0.866072  0.286959         0.717146\n",
        "162470  0.225753  0.222634  0.261150  0.314630         0.113323\n",
        "114899  0.341813  0.335036  0.579324  0.351410         0.760790\n",
        "35726   0.359572  0.352251  0.566274  0.361131         0.173213\n",
        "\n",
        "[25044 rows x 322 columns]\n",
        "Top predictors based on filtered model:\n",
        "Index(['cat80_D', 'cat80_B', 'cat57_B', 'cat79_D', 'cat12_B', 'cat113_U',\n",
        "       'cat105_P', 'cat87_B', 'cat81_D', 'cat106_C',\n",
        "       ...\n",
        "       'cat104_C', 'cat66_B', 'cat112_F', 'cat116_CB', 'cat116_HB', 'cat100_B',\n",
        "       'cat110_N', 'cat116_LQ', 'cat112_AR', 'uniform_feature'],\n",
        "      dtype='object', length=322)\n",
        "# Plot Predicted vs Actual values for visualization with filtered model\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_filtered, alpha=0.5, label=\"Predicted vs Actual (Filtered)\")\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label=\"Ideal Fit Line\")\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values (Filtered Model)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Extract and print the top 20 most important features from the feature importance chart\n",
        "top_20_indices = np.argsort(importances)[::-1][:50]  # Get the top 20 indices\n",
        "important_features = features[top_20_indices]  # Select the top 20 features\n",
        "print(\"Top 50 most important features:\")\n",
        "print(important_features)\n",
        "Top 50 most important features:\n",
        "Index(['cat80_D', 'cat80_B', 'cat57_B', 'cat79_D', 'cat12_B', 'cat113_U',\n",
        "       'cat105_P', 'cat87_B', 'cat81_D', 'cat106_C', 'cat116_KP', 'cat107_D',\n",
        "       'cat10_B', 'cont7', 'cont12', 'cat115_E', 'cont2', 'cat109_BI',\n",
        "       'cat53_B', 'cat76_B', 'cat72_B', 'cat113_G', 'cat7_B', 'cat26_B',\n",
        "       'cat116_J', 'cat38_B', 'cat44_B', 'cat61_B', 'cat100_I', 'cat113_AL',\n",
        "       'cat11_B', 'cat82_D', 'cat1_B', 'cat13_B', 'cat100_G', 'cat113_AT',\n",
        "       'cat116_U', 'cat105_T', 'cat27_B', 'cat109_AB', 'cat52_B', 'cat9_B',\n",
        "       'cat49_B', 'cat108_K', 'cat116_W', 'cat115_H', 'cat91_C', 'cat79_B',\n",
        "       'cat37_B', 'cat103_G'],\n",
        "      dtype='object')\n",
        "# Function to train and evaluate the model using a subset of top features\n",
        "def train_and_evaluate_model(top_n):\n",
        "    # Select the top 'top_n' features\n",
        "    selected_features = features[indices[:top_n]]\n",
        "    X_train_top = X_train[selected_features]\n",
        "    X_test_top = X_test[selected_features]\n",
        "\n",
        "    # Train the model with the selected features\n",
        "    model_top = xgb.XGBRegressor(**best_params)\n",
        "    model_top.fit(X_train_top, y_train)\n",
        "\n",
        "    # Make predictions using the model with the selected features\n",
        "    y_pred_top = model_top.predict(X_test_top)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mse_top = mean_squared_error(y_test, y_pred_top)\n",
        "    rmse_top = np.sqrt(mse_top)\n",
        "    r2_top = r2_score(y_test, y_pred_top)\n",
        "\n",
        "    print(f\"Performance with top {top_n} features:\")\n",
        "    print(f\"Mean Squared Error: {mse_top}\")\n",
        "    print(f\"Root Mean Squared Error: {rmse_top}\")\n",
        "    print(f\"R²: {r2_top}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Retrain and evaluate the model with top 20, top 50, top 10, and top 5 features\n",
        "train_and_evaluate_model(50)\n",
        "train_and_evaluate_model(20)\n",
        "train_and_evaluate_model(10)\n",
        "train_and_evaluate_model(5)\n",
        ""
      ],
      "metadata": {
        "id": "3tawlqBBRDxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entire Thing"
      ],
      "metadata": {
        "id": "BA8SEXLaRSER"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qJ7VOjGRTcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}